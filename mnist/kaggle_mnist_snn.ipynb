{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c digit-recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ C Ё¬ҐҐв ¬ҐвЄг Acer\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : 0A91-591B\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ C:\\Users\\qu4n7\\Desktop\\neural_network\\_2\n",
      "\n",
      "25.06.2019  17:50    <DIR>          .\n",
      "25.06.2019  17:50    <DIR>          ..\n",
      "24.06.2019  17:23    <DIR>          .ipynb_checkpoints\n",
      "12.06.2019  12:10            20я899 0_neural_course_02_kaggle_mnist.ipynb\n",
      "12.06.2019  10:07            69я644 0_neural_course_02_overfitting.ipynb\n",
      "24.06.2019  22:33           374я492 neural_course_02_kaggle_mnist.ipynb\n",
      "25.06.2019  17:50           223я784 neural_course_02_kaggle_mnist.light.¤§.ipynb\n",
      "12.06.2019  12:10            34я277 neural_course_02_overfitting.ipynb\n",
      "15.06.2019  21:29            24я045 pima-indians-diabetes.csv\n",
      "13.06.2019  10:57           240я909 sample_submission.csv\n",
      "24.06.2019  22:51           240я909 submission.csv\n",
      "13.06.2019  10:57        51я118я296 test.csv\n",
      "13.06.2019  10:57        76я775я041 train.csv\n",
      "              10 д ©«®ў    129я122я296 Ў ©в\n",
      "               3 Ї Ї®Є  71я576я403я968 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 784)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      0\n",
       "1        2      0\n",
       "2        3      0\n",
       "3        4      0\n",
       "4        5      0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [7, 0, 0, ..., 0, 0, 0],\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       [9, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = df_train.values\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data <= neural networks works much better if training set is within [0,1]\n",
    "x_train = x_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_dataset[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the draft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input subsequent layer, 800 neurons, 784 inputs for the each neuron \n",
    "model.add(Dense(800, input_dim=784, activation='relu'))\n",
    "# output subsequent layer, 10 neurons\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 800)               628000    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 636,010\n",
      "Trainable params: 636,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# using categorical_crossentropy since the function is differentiable, this is must for the  \n",
    "# gradient descent algorithms 'adam'\n",
    "# genetic algorithms allow others \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/30\n",
      "33600/33600 [==============================] - 3s 80us/sample - loss: 0.3527 - acc: 0.9000 - val_loss: 0.1849 - val_acc: 0.9492\n",
      "Epoch 2/30\n",
      "33600/33600 [==============================] - 2s 68us/sample - loss: 0.1463 - acc: 0.9589 - val_loss: 0.1333 - val_acc: 0.9602\n",
      "Epoch 3/30\n",
      "33600/33600 [==============================] - 2s 68us/sample - loss: 0.0986 - acc: 0.9714 - val_loss: 0.1085 - val_acc: 0.9664\n",
      "Epoch 4/30\n",
      "33600/33600 [==============================] - 2s 68us/sample - loss: 0.0700 - acc: 0.9805 - val_loss: 0.1020 - val_acc: 0.9676\n",
      "Epoch 5/30\n",
      "33600/33600 [==============================] - 2s 68us/sample - loss: 0.0506 - acc: 0.9868 - val_loss: 0.0887 - val_acc: 0.9715\n",
      "Epoch 6/30\n",
      "33600/33600 [==============================] - 2s 69us/sample - loss: 0.0367 - acc: 0.9907 - val_loss: 0.0866 - val_acc: 0.9731\n",
      "Epoch 7/30\n",
      "33600/33600 [==============================] - 2s 67us/sample - loss: 0.0271 - acc: 0.9937 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 8/30\n",
      "33600/33600 [==============================] - 2s 69us/sample - loss: 0.0211 - acc: 0.9955 - val_loss: 0.0817 - val_acc: 0.9739\n",
      "Epoch 9/30\n",
      "33600/33600 [==============================] - 2s 68us/sample - loss: 0.0151 - acc: 0.9975 - val_loss: 0.0803 - val_acc: 0.9762\n",
      "Epoch 10/30\n",
      "33600/33600 [==============================] - 2s 70us/sample - loss: 0.0108 - acc: 0.9987 - val_loss: 0.0845 - val_acc: 0.9751\n",
      "Epoch 11/30\n",
      "33600/33600 [==============================] - 2s 70us/sample - loss: 0.0086 - acc: 0.9992 - val_loss: 0.0798 - val_acc: 0.9762\n",
      "Epoch 12/30\n",
      "33600/33600 [==============================] - 2s 69us/sample - loss: 0.0063 - acc: 0.9993 - val_loss: 0.0812 - val_acc: 0.9765\n",
      "Epoch 13/30\n",
      "33600/33600 [==============================] - 2s 70us/sample - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0823 - val_acc: 0.9776\n",
      "Epoch 14/30\n",
      "33600/33600 [==============================] - 2s 69us/sample - loss: 0.0040 - acc: 0.9998 - val_loss: 0.0778 - val_acc: 0.9786\n",
      "Epoch 15/30\n",
      "33600/33600 [==============================] - 3s 75us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0803 - val_acc: 0.9779\n",
      "Epoch 16/30\n",
      "33600/33600 [==============================] - 2s 72us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0801 - val_acc: 0.9798\n",
      "Epoch 17/30\n",
      "33600/33600 [==============================] - 2s 73us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9793\n",
      "Epoch 18/30\n",
      "33600/33600 [==============================] - 2s 72us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9793\n",
      "Epoch 19/30\n",
      "33600/33600 [==============================] - 2s 72us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9789\n",
      "Epoch 20/30\n",
      "33600/33600 [==============================] - 3s 79us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9785\n",
      "Epoch 21/30\n",
      "33600/33600 [==============================] - 3s 78us/sample - loss: 9.1562e-04 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9796\n",
      "Epoch 22/30\n",
      "33600/33600 [==============================] - 3s 78us/sample - loss: 8.0623e-04 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9795\n",
      "Epoch 23/30\n",
      "33600/33600 [==============================] - 3s 78us/sample - loss: 7.3619e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9801\n",
      "Epoch 24/30\n",
      "33600/33600 [==============================] - 3s 76us/sample - loss: 6.2550e-04 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9793\n",
      "Epoch 25/30\n",
      "33600/33600 [==============================] - 3s 75us/sample - loss: 5.5144e-04 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9789\n",
      "Epoch 26/30\n",
      "33600/33600 [==============================] - 2s 73us/sample - loss: 4.7898e-04 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9794\n",
      "Epoch 27/30\n",
      "33600/33600 [==============================] - 2s 71us/sample - loss: 4.3727e-04 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9795\n",
      "Epoch 28/30\n",
      "33600/33600 [==============================] - 3s 83us/sample - loss: 3.9105e-04 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9786\n",
      "Epoch 29/30\n",
      "33600/33600 [==============================] - 3s 76us/sample - loss: 3.5499e-04 - acc: 1.0000 - val_loss: 0.0919 - val_acc: 0.9781\n",
      "Epoch 30/30\n",
      "33600/33600 [==============================] - 2s 74us/sample - loss: 3.1620e-04 - acc: 1.0000 - val_loss: 0.0903 - val_acc: 0.9799\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=200, epochs=30, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(history.history['acc'], label='accuracy score @ train')\n",
    "plt.plot(history.history['val_acc'], label='accuracy score @ test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = np.loadtxt('test.csv', skiprows=1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_dataset / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.12824804e-14, 2.14217348e-24, 1.00000000e+00, 8.74618185e-15,\n",
       "        1.76769328e-15, 5.23601547e-20, 4.49144250e-16, 6.79694601e-15,\n",
       "        3.40620027e-15, 3.44684590e-18],\n",
       "       [1.00000000e+00, 1.73026334e-14, 2.21476982e-12, 1.68980639e-12,\n",
       "        5.54314243e-17, 1.17253873e-09, 1.15447517e-12, 9.24201774e-12,\n",
       "        4.14670570e-14, 3.28316683e-15],\n",
       "       [7.43317699e-11, 2.87885964e-08, 4.52193717e-07, 4.95905479e-05,\n",
       "        2.37982531e-05, 4.44369959e-07, 2.06401368e-10, 1.62472804e-06,\n",
       "        1.89668190e-05, 9.99905109e-01],\n",
       "       [1.17859235e-02, 2.36972508e-11, 2.36787644e-04, 9.45464251e-07,\n",
       "        9.62878403e-04, 9.37602973e-09, 1.84859755e-06, 1.43875991e-06,\n",
       "        8.61031921e-08, 9.87010121e-01],\n",
       "       [1.31872443e-12, 4.82906548e-10, 4.58857085e-06, 9.99991059e-01,\n",
       "        9.23302205e-19, 1.93877966e-07, 9.24852534e-13, 3.18541444e-13,\n",
       "        4.11526344e-06, 5.51255708e-10]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, 9, 3], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kaggle submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.column_stack((range(1, predictions.shape[0]+1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 0],\n",
       "       [3, 9],\n",
       "       [4, 9],\n",
       "       [5, 3]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('submission.csv', out, header='ImageId,Label', comments='', fmt='%d,%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = pd.read_csv('submission.csv')\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c digit-recognizer -m \"submission_190613\" -f submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search for the network hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning batch size and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing function for creating a model\n",
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(800, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [30, 40, 50, 70]\n",
    "epochs = [10, 50, 80]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 9s 255us/sample - loss: 0.2366 - acc: 0.9293 - val_loss: 0.1161 - val_acc: 0.9658\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0933 - acc: 0.9723 - val_loss: 0.1035 - val_acc: 0.9671\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0585 - acc: 0.9820 - val_loss: 0.1001 - val_acc: 0.9694\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 8s 243us/sample - loss: 0.0399 - acc: 0.9870 - val_loss: 0.0927 - val_acc: 0.9720\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 8s 245us/sample - loss: 0.0284 - acc: 0.9905 - val_loss: 0.0890 - val_acc: 0.9771\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0958 - val_acc: 0.9744\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0993 - val_acc: 0.9756\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0162 - acc: 0.9946 - val_loss: 0.0997 - val_acc: 0.9773\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 8s 241us/sample - loss: 0.0140 - acc: 0.9957 - val_loss: 0.0969 - val_acc: 0.9770\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 8s 241us/sample - loss: 0.0129 - acc: 0.9956 - val_loss: 0.1006 - val_acc: 0.9783\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 8s 245us/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.1458 - val_acc: 0.9698\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0086 - acc: 0.9974 - val_loss: 0.1179 - val_acc: 0.9764\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 8s 240us/sample - loss: 0.0119 - acc: 0.9962 - val_loss: 0.1150 - val_acc: 0.9758\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0074 - acc: 0.9975 - val_loss: 0.1109 - val_acc: 0.9781\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.1251 - val_acc: 0.9755\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 8s 246us/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.1240 - val_acc: 0.9768\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 8s 245us/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1241 - val_acc: 0.9786\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 8s 241us/sample - loss: 0.0086 - acc: 0.9973 - val_loss: 0.1405 - val_acc: 0.9756\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.1337 - val_acc: 0.9777\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 8s 243us/sample - loss: 0.0040 - acc: 0.9986 - val_loss: 0.1365 - val_acc: 0.9770\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 8s 240us/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.1677 - val_acc: 0.9721\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 8s 240us/sample - loss: 0.0061 - acc: 0.9974 - val_loss: 0.1327 - val_acc: 0.9773\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 8s 245us/sample - loss: 0.0031 - acc: 0.9989 - val_loss: 0.1332 - val_acc: 0.9798\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 8s 240us/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1876 - val_acc: 0.9690\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0076 - acc: 0.9977 - val_loss: 0.1678 - val_acc: 0.9748\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 8s 239us/sample - loss: 0.0062 - acc: 0.9978 - val_loss: 0.1473 - val_acc: 0.9755\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0047 - acc: 0.9985 - val_loss: 0.1595 - val_acc: 0.9765\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 8s 241us/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1473 - val_acc: 0.9790\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1432 - val_acc: 0.9783\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 8s 238us/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1565 - val_acc: 0.9765\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 8s 238us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.1606 - val_acc: 0.9775\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 8s 243us/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1541 - val_acc: 0.9792\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 8s 245us/sample - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1463 - val_acc: 0.9776\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0055 - acc: 0.9987 - val_loss: 0.1498 - val_acc: 0.9795\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 8s 240us/sample - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1583 - val_acc: 0.9786\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.1482 - val_acc: 0.9795\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0060 - acc: 0.9984 - val_loss: 0.1674 - val_acc: 0.9781\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1597 - val_acc: 0.9796\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 8.3855e-04 - acc: 0.9997 - val_loss: 0.1387 - val_acc: 0.9804\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1641 - val_acc: 0.9783\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0111 - acc: 0.9973 - val_loss: 0.1775 - val_acc: 0.9776\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 0.1636 - val_acc: 0.9776\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.1838 - val_acc: 0.9785\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0067 - acc: 0.9987 - val_loss: 0.1601 - val_acc: 0.9798\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0019 - acc: 0.9994 - val_loss: 0.1774 - val_acc: 0.9774\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.2121 - val_acc: 0.9749\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1951 - val_acc: 0.9758\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 8s 247us/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1932 - val_acc: 0.9770\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 8s 236us/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1897 - val_acc: 0.9773\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 8s 233us/sample - loss: 0.0052 - acc: 0.9987 - val_loss: 0.1842 - val_acc: 0.9786\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 8s 242us/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.1983 - val_acc: 0.9769\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 9s 262us/sample - loss: 0.0037 - acc: 0.9991 - val_loss: 0.1868 - val_acc: 0.9796\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 9s 263us/sample - loss: 0.0048 - acc: 0.9989 - val_loss: 0.1836 - val_acc: 0.9777\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 8s 243us/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2035 - val_acc: 0.9769\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 8s 241us/sample - loss: 0.0092 - acc: 0.9981 - val_loss: 0.1790 - val_acc: 0.9782\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 8s 238us/sample - loss: 0.0039 - acc: 0.9987 - val_loss: 0.2078 - val_acc: 0.9775\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 8s 232us/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1887 - val_acc: 0.9783\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 8s 235us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.2083 - val_acc: 0.9763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 8s 237us/sample - loss: 0.0034 - acc: 0.9992 - val_loss: 0.2157 - val_acc: 0.9771\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 8s 236us/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2317 - val_acc: 0.9744\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 8s 235us/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.2395 - val_acc: 0.9746\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 8s 232us/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1994 - val_acc: 0.9786\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 8s 244us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2178 - val_acc: 0.9760\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 8s 250us/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1965 - val_acc: 0.9786\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 266us/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.1916 - val_acc: 0.9799\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0043 - acc: 0.9990 - val_loss: 0.2333 - val_acc: 0.9750\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.2060 - val_acc: 0.9799\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.1905 - val_acc: 0.9788\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 11s 316us/sample - loss: 2.6639e-04 - acc: 0.9999 - val_loss: 0.1889 - val_acc: 0.9793\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0066 - acc: 0.9987 - val_loss: 0.2102 - val_acc: 0.9779\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.1981 - val_acc: 0.9800\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1989 - val_acc: 0.9792\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1900 - val_acc: 0.9807\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1969 - val_acc: 0.9790\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 0.0066 - acc: 0.9988 - val_loss: 0.2309 - val_acc: 0.9765\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1834 - val_acc: 0.9811\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0025 - acc: 0.9994 - val_loss: 0.2101 - val_acc: 0.9790\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.2180 - val_acc: 0.9776\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 0.0033 - acc: 0.9992 - val_loss: 0.2147 - val_acc: 0.9786\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 0.0047 - acc: 0.9994 - val_loss: 0.2087 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "grid_result_s_e = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9778095086415609\n",
      "best parameters {'batch_size': 30, 'epochs': 80}\n",
      "mean:0.9714523951212565, std:0.001047064882480838, param: {'batch_size': 30, 'epochs': 10}\n",
      "mean:0.9741904934247335, std:0.0026817188858574066, param: {'batch_size': 30, 'epochs': 50}\n",
      "mean:0.9778095086415609, std:0.0004529946894320892, param: {'batch_size': 30, 'epochs': 80}\n",
      "mean:0.9702619115511576, std:0.0017538499560145132, param: {'batch_size': 40, 'epochs': 10}\n",
      "mean:0.9776904781659445, std:0.00029355217693673895, param: {'batch_size': 40, 'epochs': 50}\n",
      "mean:0.9770238002141317, std:0.00044159213472880725, param: {'batch_size': 40, 'epochs': 80}\n",
      "mean:0.9676904678344727, std:0.003969830527567706, param: {'batch_size': 50, 'epochs': 10}\n",
      "mean:0.9766904711723328, std:0.0019746064654945854, param: {'batch_size': 50, 'epochs': 50}\n",
      "mean:0.9777142802874247, std:0.0009165892819860083, param: {'batch_size': 50, 'epochs': 80}\n",
      "mean:0.9753571550051371, std:0.0007846369479592251, param: {'batch_size': 70, 'epochs': 10}\n",
      "mean:0.9771190683046976, std:0.0003969893683448023, param: {'batch_size': 70, 'epochs': 50}\n",
      "mean:0.9767380952835083, std:0.0005957131188884606, param: {'batch_size': 70, 'epochs': 80}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_s_e.best_score_}\\nbest parameters {grid_result_s_e.best_params_}')\n",
    "means_s_e = grid_result_s_e.cv_results_['mean_test_score']\n",
    "stds_s_e = grid_result_s_e.cv_results_['std_test_score']\n",
    "params_s_e = grid_result_s_e.cv_results_['params']\n",
    "for mean, std, param in zip(means_s_e, stds_s_e, params_s_e):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size : 30\n",
    "\n",
    "epochs : 80\n",
    "\n",
    "best score : 0.9778095086415609\n",
    "\n",
    "loss: 0.0047 - acc: 0.9994 - val_loss: 0.2087 - val_acc: 0.9800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tuning optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 9s 267us/sample - loss: 0.2391 - acc: 0.9290 - val_loss: 0.1375 - val_acc: 0.9585\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0955 - acc: 0.9709 - val_loss: 0.1128 - val_acc: 0.9652\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0581 - acc: 0.9822 - val_loss: 0.0937 - val_acc: 0.9705\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0382 - acc: 0.9877 - val_loss: 0.0891 - val_acc: 0.9739\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0908 - val_acc: 0.9748\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0211 - acc: 0.9933 - val_loss: 0.0970 - val_acc: 0.9736\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0186 - acc: 0.9935 - val_loss: 0.1134 - val_acc: 0.9713\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0157 - acc: 0.9947 - val_loss: 0.1077 - val_acc: 0.9742\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0138 - acc: 0.9952 - val_loss: 0.1080 - val_acc: 0.9745\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0108 - acc: 0.9963 - val_loss: 0.1114 - val_acc: 0.9757\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0115 - acc: 0.9958 - val_loss: 0.1208 - val_acc: 0.9748\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0968 - val_acc: 0.9787\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1192 - val_acc: 0.9745\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1381 - val_acc: 0.9732\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0088 - acc: 0.9974 - val_loss: 0.1404 - val_acc: 0.9736\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0083 - acc: 0.9971 - val_loss: 0.1379 - val_acc: 0.9745\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0058 - acc: 0.9979 - val_loss: 0.1428 - val_acc: 0.9754\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.1567 - val_acc: 0.9738\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0070 - acc: 0.9977 - val_loss: 0.1419 - val_acc: 0.9754\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1179 - val_acc: 0.9806\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.1764 - val_acc: 0.9705\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0049 - acc: 0.9983 - val_loss: 0.1367 - val_acc: 0.9780\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1768 - val_acc: 0.9726\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 8s 248us/sample - loss: 0.0078 - acc: 0.9977 - val_loss: 0.1524 - val_acc: 0.9760\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1435 - val_acc: 0.9795\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.1744 - val_acc: 0.9765\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.1699 - val_acc: 0.9763\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0067 - acc: 0.9979 - val_loss: 0.1515 - val_acc: 0.9776\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1575 - val_acc: 0.9783\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1690 - val_acc: 0.9765\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1768 - val_acc: 0.9757\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 0.1850 - val_acc: 0.9756\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1791 - val_acc: 0.9763\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0054 - acc: 0.9987 - val_loss: 0.1740 - val_acc: 0.9768\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0069 - acc: 0.9981 - val_loss: 0.2259 - val_acc: 0.9727\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 0.0061 - acc: 0.9979 - val_loss: 0.1688 - val_acc: 0.9773\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1532 - val_acc: 0.9802\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 4.9180e-04 - acc: 0.9998 - val_loss: 0.1536 - val_acc: 0.9804\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 8.4046e-04 - acc: 0.9999 - val_loss: 0.1522 - val_acc: 0.9811\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.1740e-05 - acc: 1.0000 - val_loss: 0.1484 - val_acc: 0.9813\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 3.2069e-06 - acc: 1.0000 - val_loss: 0.1479 - val_acc: 0.9812\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.9708e-06 - acc: 1.0000 - val_loss: 0.1477 - val_acc: 0.9812\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.4694e-06 - acc: 1.0000 - val_loss: 0.1475 - val_acc: 0.9814\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.1049e-06 - acc: 1.0000 - val_loss: 0.1478 - val_acc: 0.9817\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 8.1863e-07 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 0.9818\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 6.0272e-07 - acc: 1.0000 - val_loss: 0.1483 - val_acc: 0.9820\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 4.4628e-07 - acc: 1.0000 - val_loss: 0.1493 - val_acc: 0.9821\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 3.4146e-07 - acc: 1.0000 - val_loss: 0.1504 - val_acc: 0.9820\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 2.6920e-07 - acc: 1.0000 - val_loss: 0.1511 - val_acc: 0.9824\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 2.1804e-07 - acc: 1.0000 - val_loss: 0.1518 - val_acc: 0.9821\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.8699e-07 - acc: 1.0000 - val_loss: 0.1526 - val_acc: 0.9819\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.6470e-07 - acc: 1.0000 - val_loss: 0.1536 - val_acc: 0.9821\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.5247e-07 - acc: 1.0000 - val_loss: 0.1554 - val_acc: 0.9820\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.4045e-07 - acc: 1.0000 - val_loss: 0.1559 - val_acc: 0.9823\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.3341e-07 - acc: 1.0000 - val_loss: 0.1568 - val_acc: 0.9824\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.2929e-07 - acc: 1.0000 - val_loss: 0.1581 - val_acc: 0.9826\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.2640e-07 - acc: 1.0000 - val_loss: 0.1588 - val_acc: 0.9826\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.2471e-07 - acc: 1.0000 - val_loss: 0.1586 - val_acc: 0.9825\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.2340e-07 - acc: 1.0000 - val_loss: 0.1602 - val_acc: 0.9825\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.2228e-07 - acc: 1.0000 - val_loss: 0.1603 - val_acc: 0.9826\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.2151e-07 - acc: 1.0000 - val_loss: 0.1609 - val_acc: 0.9826\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.2106e-07 - acc: 1.0000 - val_loss: 0.1613 - val_acc: 0.9826\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.2058e-07 - acc: 1.0000 - val_loss: 0.1614 - val_acc: 0.9826\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.2037e-07 - acc: 1.0000 - val_loss: 0.1617 - val_acc: 0.9826\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.2017e-07 - acc: 1.0000 - val_loss: 0.1621 - val_acc: 0.9826\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.1996e-07 - acc: 1.0000 - val_loss: 0.1622 - val_acc: 0.9826\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.1985e-07 - acc: 1.0000 - val_loss: 0.1624 - val_acc: 0.9826\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 8s 250us/sample - loss: 1.1976e-07 - acc: 1.0000 - val_loss: 0.1627 - val_acc: 0.9826\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.1966e-07 - acc: 1.0000 - val_loss: 0.1625 - val_acc: 0.9826\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.1958e-07 - acc: 1.0000 - val_loss: 0.1627 - val_acc: 0.9826\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.1952e-07 - acc: 1.0000 - val_loss: 0.1631 - val_acc: 0.9826\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 8s 250us/sample - loss: 1.1950e-07 - acc: 1.0000 - val_loss: 0.1631 - val_acc: 0.9826\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.1944e-07 - acc: 1.0000 - val_loss: 0.1634 - val_acc: 0.9826\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.1943e-07 - acc: 1.0000 - val_loss: 0.1632 - val_acc: 0.9826\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.1939e-07 - acc: 1.0000 - val_loss: 0.1633 - val_acc: 0.9826\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.1936e-07 - acc: 1.0000 - val_loss: 0.1636 - val_acc: 0.9826\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 9s 254us/sample - loss: 1.1934e-07 - acc: 1.0000 - val_loss: 0.1636 - val_acc: 0.9825\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 9s 253us/sample - loss: 1.1932e-07 - acc: 1.0000 - val_loss: 0.1637 - val_acc: 0.9825\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 8s 253us/sample - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 0.1637 - val_acc: 0.9826\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 8s 252us/sample - loss: 1.1928e-07 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(800, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_a = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9780714313189188\n",
      "best parameters {'optimizer': 'Adadelta'}\n",
      "mean:0.9776190519332886, std:0.0015868638381951329, param: {'optimizer': 'SGD'}\n",
      "mean:0.9768333236376444, std:0.0016916587929197134, param: {'optimizer': 'RMSprop'}\n",
      "mean:0.9775476257006327, std:0.0005841957777721462, param: {'optimizer': 'Adagrad'}\n",
      "mean:0.9780714313189188, std:0.0003499399822886081, param: {'optimizer': 'Adadelta'}\n",
      "mean:0.9771190484364828, std:0.0017518910101985896, param: {'optimizer': 'Adam'}\n",
      "mean:0.9776190519332886, std:0.000587101129894498, param: {'optimizer': 'Adamax'}\n",
      "mean:0.9763571619987488, std:0.002838037070625585, param: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_a.best_score_}\\nbest parameters {grid_result_a.best_params_}')\n",
    "means_a = grid_result_a.cv_results_['mean_test_score']\n",
    "stds_a = grid_result_a.cv_results_['std_test_score']\n",
    "params_a = grid_result_a.cv_results_['params']\n",
    "for mean, std, param in zip(means_a, stds_a, params_a):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer : Adadelta\n",
    "\n",
    "best score : 0.0.9780714313189188\n",
    "\n",
    "loss: 1.1928e-07 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 0.9826"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tuning network weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 10s 290us/sample - loss: 0.2612 - acc: 0.9206 - val_loss: 0.1335 - val_acc: 0.9610\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.1054 - acc: 0.9699 - val_loss: 0.1010 - val_acc: 0.9674\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0677 - acc: 0.9797 - val_loss: 0.0918 - val_acc: 0.9725\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 0.0488 - acc: 0.9861 - val_loss: 0.0763 - val_acc: 0.9765\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0340 - acc: 0.9907 - val_loss: 0.0850 - val_acc: 0.9745\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0239 - acc: 0.9936 - val_loss: 0.0781 - val_acc: 0.9764\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 0.0174 - acc: 0.9960 - val_loss: 0.0780 - val_acc: 0.9786\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0121 - acc: 0.9972 - val_loss: 0.0778 - val_acc: 0.9773\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0086 - acc: 0.9982 - val_loss: 0.0790 - val_acc: 0.9786\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0066 - acc: 0.9987 - val_loss: 0.0737 - val_acc: 0.9794\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.0795 - val_acc: 0.9786\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.0799 - val_acc: 0.9798\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0790 - val_acc: 0.9793\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0806 - val_acc: 0.9790\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0803 - val_acc: 0.9802\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 8.6018e-04 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9804\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 5.9827e-04 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9805\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 4.9629e-04 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9805\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 4.1375e-04 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9800\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 3.4684e-04 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 0.9802\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 3.0257e-04 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9799\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 2.8220e-04 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9804\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 2.4716e-04 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 0.9800\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 2.2658e-04 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9804\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 2.0977e-04 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9801\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 1.9363e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9805\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 1.8189e-04 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9808\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 1.7072e-04 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9800\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 1.6022e-04 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9804\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 1.5113e-04 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9807\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 1.4326e-04 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9805\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 1.3666e-04 - acc: 1.0000 - val_loss: 0.0899 - val_acc: 0.9805\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 1.2931e-04 - acc: 1.0000 - val_loss: 0.0902 - val_acc: 0.9802\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 9s 273us/sample - loss: 1.2427e-04 - acc: 1.0000 - val_loss: 0.0907 - val_acc: 0.9802\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 1.1883e-04 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9804\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 1.1379e-04 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9802\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 1.0921e-04 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9804\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 1.0496e-04 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9802\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 9s 273us/sample - loss: 1.0131e-04 - acc: 1.0000 - val_loss: 0.0921 - val_acc: 0.9801\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 9.7650e-05 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9802\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 9.4147e-05 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9804\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 9.0921e-05 - acc: 1.0000 - val_loss: 0.0926 - val_acc: 0.9804\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 9s 271us/sample - loss: 8.7657e-05 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9801\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 8.5375e-05 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9802\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 8.2644e-05 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9802\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 8.0000e-05 - acc: 1.0000 - val_loss: 0.0934 - val_acc: 0.9804\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 7.7731e-05 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 0.9801\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 7.5435e-05 - acc: 1.0000 - val_loss: 0.0937 - val_acc: 0.9805\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 7.3423e-05 - acc: 1.0000 - val_loss: 0.0940 - val_acc: 0.9802\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 7.1520e-05 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9802\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 9s 273us/sample - loss: 6.9552e-05 - acc: 1.0000 - val_loss: 0.0942 - val_acc: 0.9804\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 6.7843e-05 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9802\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 6.6332e-05 - acc: 1.0000 - val_loss: 0.0947 - val_acc: 0.9801\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 6.4445e-05 - acc: 1.0000 - val_loss: 0.0947 - val_acc: 0.9802\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 6.2998e-05 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9801\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 9s 268us/sample - loss: 6.1579e-05 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9802\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 6.0113e-05 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9802\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 9s 273us/sample - loss: 5.8783e-05 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9802\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 5.7602e-05 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9802\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 5.6311e-05 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9801\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 5.5025e-05 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9801\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 5.3721e-05 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9800\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 5.2997e-05 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9802\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 5.1727e-05 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9801\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 5.0837e-05 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9801\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 9s 270us/sample - loss: 4.9896e-05 - acc: 1.0000 - val_loss: 0.0967 - val_acc: 0.9800\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 4.8957e-05 - acc: 1.0000 - val_loss: 0.0968 - val_acc: 0.9802\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.8040e-05 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9801\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.7122e-05 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9801\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 9s 271us/sample - loss: 4.6260e-05 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9802\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.5450e-05 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9801\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 4.4720e-05 - acc: 1.0000 - val_loss: 0.0973 - val_acc: 0.9801\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.3799e-05 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9801\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.3330e-05 - acc: 1.0000 - val_loss: 0.0976 - val_acc: 0.9802\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 4.2490e-05 - acc: 1.0000 - val_loss: 0.0976 - val_acc: 0.9801\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 4.1867e-05 - acc: 1.0000 - val_loss: 0.0977 - val_acc: 0.9801\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 4.1173e-05 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9801\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 4.0466e-05 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9801\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 9s 272us/sample - loss: 3.9915e-05 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 0.9801\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 3.9300e-05 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "def create_model(init_mode='softmax'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, kernel_initializer=init_mode, activation='relu'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_w = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9764761924743652\n",
      "best parameters {'init_mode': 'normal'}\n",
      "mean:0.9759761889775594, std:0.0003515336903624437, param: {'init_mode': 'uniform'}\n",
      "mean:0.9751428564389547, std:0.0012710819245906654, param: {'init_mode': 'lecun_uniform'}\n",
      "mean:0.9764761924743652, std:0.0007055121917699546, param: {'init_mode': 'normal'}\n",
      "mean:0.11152380704879761, std:0.0014827040168806835, param: {'init_mode': 'zero'}\n",
      "mean:0.9760000109672546, std:0.0007715344084274748, param: {'init_mode': 'glorot_normal'}\n",
      "mean:0.9760476152102152, std:0.0005753719756120067, param: {'init_mode': 'glorot_uniform'}\n",
      "mean:0.9754523634910583, std:0.0006734223455070648, param: {'init_mode': 'he_normal'}\n",
      "mean:0.9757619102795919, std:0.0008908911967893226, param: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_w.best_score_}\\nbest parameters {grid_result_w.best_params_}')\n",
    "means_w = grid_result_w.cv_results_['mean_test_score']\n",
    "stds_w = grid_result_w.cv_results_['std_test_score']\n",
    "params_w = grid_result_w.cv_results_['params']\n",
    "for mean, std, param in zip(means_w, stds_w, params_w):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.2560 - acc: 0.9240 - val_loss: 0.1273 - val_acc: 0.9620\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.1050 - acc: 0.9694 - val_loss: 0.0997 - val_acc: 0.9682\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0674 - acc: 0.9797 - val_loss: 0.0973 - val_acc: 0.9725\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0484 - acc: 0.9859 - val_loss: 0.0805 - val_acc: 0.9754\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 0.0342 - acc: 0.9904 - val_loss: 0.0847 - val_acc: 0.9749\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0237 - acc: 0.9941 - val_loss: 0.0772 - val_acc: 0.9768\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0170 - acc: 0.9958 - val_loss: 0.0780 - val_acc: 0.9779\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0855 - val_acc: 0.9777\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0092 - acc: 0.9981 - val_loss: 0.0788 - val_acc: 0.9792\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0061 - acc: 0.9989 - val_loss: 0.0803 - val_acc: 0.9787\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.0805 - val_acc: 0.9790\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0805 - val_acc: 0.9799\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0828 - val_acc: 0.9790\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0835 - val_acc: 0.9811\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0846 - val_acc: 0.9801\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 7.3485e-04 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9810\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 9s 275us/sample - loss: 5.8596e-04 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9802\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 4.4733e-04 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9814\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 3.8861e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9805\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 3.4038e-04 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9812\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 2.9667e-04 - acc: 1.0000 - val_loss: 0.0883 - val_acc: 0.9810\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 2.7328e-04 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9808\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 2.4564e-04 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9802\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 2.2818e-04 - acc: 1.0000 - val_loss: 0.0899 - val_acc: 0.9807\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 2.0887e-04 - acc: 1.0000 - val_loss: 0.0902 - val_acc: 0.9814\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 1.9251e-04 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9810\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 1.8109e-04 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9812\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 1.6935e-04 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 0.9818\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 1.5978e-04 - acc: 1.0000 - val_loss: 0.0924 - val_acc: 0.9807\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 1.5051e-04 - acc: 1.0000 - val_loss: 0.0922 - val_acc: 0.9812\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 1.4314e-04 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9811\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 1.3505e-04 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9811\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 1.2948e-04 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9817\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 1.2354e-04 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9807\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 1.1837e-04 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9814\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 1.1331e-04 - acc: 1.0000 - val_loss: 0.0948 - val_acc: 0.9813\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 9s 277us/sample - loss: 1.0903e-04 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9807\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 1.0503e-04 - acc: 1.0000 - val_loss: 0.0959 - val_acc: 0.9810\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 1.0073e-04 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9810\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 9.7359e-05 - acc: 1.0000 - val_loss: 0.0962 - val_acc: 0.9808\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.2920e-05 - acc: 1.0000 - val_loss: 0.1007 - val_acc: 0.9812\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 5.1766e-05 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9812\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 5.0843e-05 - acc: 1.0000 - val_loss: 0.1008 - val_acc: 0.9810\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 9s 276us/sample - loss: 4.9821e-05 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9810\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 4.8943e-05 - acc: 1.0000 - val_loss: 0.1012 - val_acc: 0.9814\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 4.8090e-05 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 0.9815\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 3.9889e-05 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9810\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 3.9176e-05 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9812\n"
     ]
    }
   ],
   "source": [
    "def create_model(init_mode='softmax'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(10,  kernel_initializer=init_mode, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_w = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9764285882314047\n",
      "best parameters {'init_mode': 'he_uniform'}\n",
      "mean:0.976285715897878, std:0.001011844145464149, param: {'init_mode': 'uniform'}\n",
      "mean:0.9755476117134094, std:0.0007961046147928391, param: {'init_mode': 'lecun_uniform'}\n",
      "mean:0.9756666620572408, std:0.000729194604394213, param: {'init_mode': 'normal'}\n",
      "mean:0.9756666620572408, std:0.00043774257211227983, param: {'init_mode': 'zero'}\n",
      "mean:0.9755476117134094, std:0.0011132192016632522, param: {'init_mode': 'glorot_normal'}\n",
      "mean:0.9758333365122477, std:0.0007198188257217453, param: {'init_mode': 'glorot_uniform'}\n",
      "mean:0.975523829460144, std:0.0008438173080030853, param: {'init_mode': 'he_normal'}\n",
      "mean:0.9764285882314047, std:0.0006725861597694601, param: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_w.best_score_}\\nbest parameters {grid_result_w.best_params_}')\n",
    "means_w = grid_result_w.cv_results_['mean_test_score']\n",
    "stds_w = grid_result_w.cv_results_['std_test_score']\n",
    "params_w = grid_result_w.cv_results_['params']\n",
    "for mean, std, param in zip(means_w, stds_w, params_w):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'init_mode': 'he_uniform'\n",
    "\n",
    "best score : 0.9764285882314047\n",
    "\n",
    "loss: 3.9176e-05 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 11s 316us/sample - loss: 0.2563 - acc: 0.9231 - val_loss: 0.1467 - val_acc: 0.9538\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 0.1063 - acc: 0.9689 - val_loss: 0.1028 - val_acc: 0.9671\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 10s 299us/sample - loss: 0.0706 - acc: 0.9789 - val_loss: 0.0841 - val_acc: 0.9735\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 10s 304us/sample - loss: 0.0501 - acc: 0.9853 - val_loss: 0.0774 - val_acc: 0.9767\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.0356 - acc: 0.9901 - val_loss: 0.0764 - val_acc: 0.9757\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 0.0254 - acc: 0.9933 - val_loss: 0.0786 - val_acc: 0.9770\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0796 - val_acc: 0.9774\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.0135 - acc: 0.9966 - val_loss: 0.0794 - val_acc: 0.9781\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0835 - val_acc: 0.9767\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.0069 - acc: 0.9987 - val_loss: 0.0722 - val_acc: 0.9800\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0745 - val_acc: 0.9794\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 0.0033 - acc: 0.9996 - val_loss: 0.0779 - val_acc: 0.9796\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0785 - val_acc: 0.9805\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 10s 308us/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0810 - val_acc: 0.9796\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 10s 306us/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0837 - val_acc: 0.9800\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 10s 304us/sample - loss: 8.4822e-04 - acc: 0.9999 - val_loss: 0.0807 - val_acc: 0.9801\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 6.2296e-04 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9799\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 5.1870e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9793\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 10s 306us/sample - loss: 4.2971e-04 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9804\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 10s 307us/sample - loss: 3.5720e-04 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9802\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 3.1087e-04 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9808\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 2.8109e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9805\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 10s 304us/sample - loss: 2.5402e-04 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9805\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 2.2987e-04 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9799\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 2.1184e-04 - acc: 1.0000 - val_loss: 0.0883 - val_acc: 0.9800\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 2.0030e-04 - acc: 1.0000 - val_loss: 0.0884 - val_acc: 0.9800\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 1.8460e-04 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9810\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 10s 299us/sample - loss: 1.7211e-04 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9807\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 10s 304us/sample - loss: 1.6263e-04 - acc: 1.0000 - val_loss: 0.0891 - val_acc: 0.9802\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 10s 310us/sample - loss: 1.5363e-04 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9805\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 1.4582e-04 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9808\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 10s 307us/sample - loss: 1.3881e-04 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 0.9806\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 10s 303us/sample - loss: 1.3119e-04 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9801\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 1.2585e-04 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9807\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 1.2076e-04 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9805\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 12s 354us/sample - loss: 1.1551e-04 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9805\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 10s 299us/sample - loss: 1.1040e-04 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9802\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 1.0650e-04 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9806\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 1.0206e-04 - acc: 1.0000 - val_loss: 0.0921 - val_acc: 0.9804\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 9.8977e-05 - acc: 1.0000 - val_loss: 0.0921 - val_acc: 0.9806\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 12s 347us/sample - loss: 9.5274e-05 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9807\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 9.1996e-05 - acc: 1.0000 - val_loss: 0.0928 - val_acc: 0.9807\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 8.9031e-05 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9805\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 10s 296us/sample - loss: 8.5909e-05 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9806\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 8.3356e-05 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 0.9805\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 10s 297us/sample - loss: 8.1101e-05 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9805\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 10s 299us/sample - loss: 7.8406e-05 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9807\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 7.6490e-05 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9807\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 10s 302us/sample - loss: 7.4215e-05 - acc: 1.0000 - val_loss: 0.0939 - val_acc: 0.9805\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 7.2381e-05 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9810\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 10s 297us/sample - loss: 7.0522e-05 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9808\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 10s 304us/sample - loss: 6.8518e-05 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9804\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 10s 297us/sample - loss: 6.6845e-05 - acc: 1.0000 - val_loss: 0.0945 - val_acc: 0.9807\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 6.5388e-05 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9805\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 10s 297us/sample - loss: 6.3683e-05 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9805\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 10s 301us/sample - loss: 6.2139e-05 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9807\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 10s 293us/sample - loss: 6.0818e-05 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9805\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 5.9361e-05 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9804\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 10s 289us/sample - loss: 5.8225e-05 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9806\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 5.6912e-05 - acc: 1.0000 - val_loss: 0.0957 - val_acc: 0.9804\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 10s 293us/sample - loss: 5.5587e-05 - acc: 1.0000 - val_loss: 0.0959 - val_acc: 0.9805\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 5.4566e-05 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9805\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 5.3557e-05 - acc: 1.0000 - val_loss: 0.0962 - val_acc: 0.9805\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 5.2413e-05 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9805\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 5.1388e-05 - acc: 1.0000 - val_loss: 0.0967 - val_acc: 0.9805\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 10s 293us/sample - loss: 5.0430e-05 - acc: 1.0000 - val_loss: 0.0967 - val_acc: 0.9805\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 10s 294us/sample - loss: 4.9485e-05 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9805\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 4.8543e-05 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9804\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 10s 292us/sample - loss: 4.7678e-05 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9805\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 10s 296us/sample - loss: 4.6761e-05 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9805\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 4.5908e-05 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9804\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 4.5149e-05 - acc: 1.0000 - val_loss: 0.0977 - val_acc: 0.9806\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 4.4440e-05 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9806\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 4.3683e-05 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9807\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 10s 295us/sample - loss: 4.2988e-05 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9804\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 10s 296us/sample - loss: 4.2308e-05 - acc: 1.0000 - val_loss: 0.0981 - val_acc: 0.9804\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 10s 296us/sample - loss: 4.1651e-05 - acc: 1.0000 - val_loss: 0.0981 - val_acc: 0.9805\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 10s 298us/sample - loss: 4.0928e-05 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9808\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 10s 299us/sample - loss: 4.0321e-05 - acc: 1.0000 - val_loss: 0.0980 - val_acc: 0.9806\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 10s 296us/sample - loss: 3.9681e-05 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "def create_model(init_mode='softmax'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, kernel_initializer=init_mode, activation='relu'))\n",
    "\tmodel.add(Dense(10, kernel_initializer=init_mode, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_w = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9762619137763977\n",
      "best parameters {'init_mode': 'glorot_uniform'}\n",
      "mean:0.9753095308939616, std:0.0011313911992116208, param: {'init_mode': 'uniform'}\n",
      "mean:0.9757142861684164, std:0.0009636411599861402, param: {'init_mode': 'lecun_uniform'}\n",
      "mean:0.9759047428766886, std:0.0006126079625017645, param: {'init_mode': 'normal'}\n",
      "mean:0.11152380704879761, std:0.0014827040168806835, param: {'init_mode': 'zero'}\n",
      "mean:0.9759523868560791, std:0.00037038650471373673, param: {'init_mode': 'glorot_normal'}\n",
      "mean:0.9762619137763977, std:0.0011403847909756811, param: {'init_mode': 'glorot_uniform'}\n",
      "mean:0.975428581237793, std:0.0007445865394734488, param: {'init_mode': 'he_normal'}\n",
      "mean:0.9751190344492594, std:0.001476944984133996, param: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_w.best_score_}\\nbest parameters {grid_result_w.best_params_}')\n",
    "means_w = grid_result_w.cv_results_['mean_test_score']\n",
    "stds_w = grid_result_w.cv_results_['std_test_score']\n",
    "params_w = grid_result_w.cv_results_['params']\n",
    "for mean, std, param in zip(means_w, stds_w, params_w):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning the neuron activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 10s 308us/sample - loss: 0.2574 - acc: 0.9235 - val_loss: 0.1450 - val_acc: 0.9555\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 10s 288us/sample - loss: 0.1068 - acc: 0.9688 - val_loss: 0.0997 - val_acc: 0.9688\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 10s 290us/sample - loss: 0.0699 - acc: 0.9802 - val_loss: 0.0931 - val_acc: 0.9714\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0492 - acc: 0.9856 - val_loss: 0.0857 - val_acc: 0.9733\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 0.0353 - acc: 0.9898 - val_loss: 0.0830 - val_acc: 0.9757\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 0.0259 - acc: 0.9929 - val_loss: 0.0805 - val_acc: 0.9750\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0187 - acc: 0.9951 - val_loss: 0.0834 - val_acc: 0.9757\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0136 - acc: 0.9968 - val_loss: 0.0796 - val_acc: 0.9769\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0813 - val_acc: 0.9789\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.0776 - val_acc: 0.9787\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0809 - val_acc: 0.9787\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0034 - acc: 0.9997 - val_loss: 0.0812 - val_acc: 0.9790\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0027 - acc: 0.9998 - val_loss: 0.0828 - val_acc: 0.9800\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 10s 287us/sample - loss: 0.0019 - acc: 0.9999 - val_loss: 0.0847 - val_acc: 0.9790\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0907 - val_acc: 0.9790\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0872 - val_acc: 0.9792\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0935 - val_acc: 0.9792\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0869 - val_acc: 0.9802\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 8.8988e-04 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 0.9798\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 8.3604e-04 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 0.9796\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 7.9713e-04 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9794\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 10s 288us/sample - loss: 7.5789e-04 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 0.9800\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 7.3539e-04 - acc: 1.0000 - val_loss: 0.0908 - val_acc: 0.9798\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 7.1312e-04 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9798\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 6.9493e-04 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9800\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 6.7557e-04 - acc: 1.0000 - val_loss: 0.0916 - val_acc: 0.9800\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.6423e-04 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 0.9795\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.5294e-04 - acc: 1.0000 - val_loss: 0.0927 - val_acc: 0.9798\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 10s 287us/sample - loss: 6.4274e-04 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9798\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.3306e-04 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9799\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 6.2511e-04 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9796\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 6.1804e-04 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9800\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 6.1139e-04 - acc: 1.0000 - val_loss: 0.0948 - val_acc: 0.9796\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.0568e-04 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9802\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 6.0051e-04 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9799\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.9516e-04 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9796\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.9028e-04 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9801\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.8606e-04 - acc: 1.0000 - val_loss: 0.0970 - val_acc: 0.9795\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.8177e-04 - acc: 1.0000 - val_loss: 0.0966 - val_acc: 0.9798\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.7886e-04 - acc: 1.0000 - val_loss: 0.0968 - val_acc: 0.9798\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.7497e-04 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9800\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.7201e-04 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9796\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.6905e-04 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9798\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.6579e-04 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9795\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.6362e-04 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9798\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.6094e-04 - acc: 1.0000 - val_loss: 0.0983 - val_acc: 0.9799\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 5.5848e-04 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9796\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.5612e-04 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9800\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.5428e-04 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 0.9799\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.5224e-04 - acc: 1.0000 - val_loss: 0.0993 - val_acc: 0.9798 loss: 5.6043e-04 - acc: 1.0\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.5056e-04 - acc: 1.0000 - val_loss: 0.0990 - val_acc: 0.9796\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.4845e-04 - acc: 1.0000 - val_loss: 0.0996 - val_acc: 0.9795\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.4670e-04 - acc: 1.0000 - val_loss: 0.0999 - val_acc: 0.9799\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.4507e-04 - acc: 1.0000 - val_loss: 0.1000 - val_acc: 0.9794\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.4355e-04 - acc: 1.0000 - val_loss: 0.1000 - val_acc: 0.9798\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.4224e-04 - acc: 1.0000 - val_loss: 0.1003 - val_acc: 0.9795\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.4075e-04 - acc: 1.0000 - val_loss: 0.1003 - val_acc: 0.9796\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.3917e-04 - acc: 1.0000 - val_loss: 0.1008 - val_acc: 0.9795\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.3813e-04 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9798\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 5.3672e-04 - acc: 1.0000 - val_loss: 0.1010 - val_acc: 0.9799\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.3566e-04 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9798\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 5.3437e-04 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 0.9798\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.3348e-04 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 0.9796\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 10s 288us/sample - loss: 5.3233e-04 - acc: 1.0000 - val_loss: 0.1016 - val_acc: 0.9798\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.3130e-04 - acc: 1.0000 - val_loss: 0.1018 - val_acc: 0.9796\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.3028e-04 - acc: 1.0000 - val_loss: 0.1020 - val_acc: 0.9798\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.2929e-04 - acc: 1.0000 - val_loss: 0.1022 - val_acc: 0.9798\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.2826e-04 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9799\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.2753e-04 - acc: 1.0000 - val_loss: 0.1024 - val_acc: 0.9796\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.2661e-04 - acc: 1.0000 - val_loss: 0.1026 - val_acc: 0.9798\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.2593e-04 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9796\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.2504e-04 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9798\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.2422e-04 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9795\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.2355e-04 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9796\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.2290e-04 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9798\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.2215e-04 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9798\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.2131e-04 - acc: 1.0000 - val_loss: 0.1034 - val_acc: 0.9798\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.2090e-04 - acc: 1.0000 - val_loss: 0.1035 - val_acc: 0.9796\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.2018e-04 - acc: 1.0000 - val_loss: 0.1038 - val_acc: 0.9796\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.1959e-04 - acc: 1.0000 - val_loss: 0.1037 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "def create_model(activation='relu'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, activation=activation))\n",
    "\tmodel.add(Dense(10,  activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_a = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9759999910990397\n",
      "best parameters {'activation': 'relu'}\n",
      "mean:0.6408095359802246, std:0.01407274276290523, param: {'activation': 'softmax'}\n",
      "mean:0.9700952370961508, std:0.0005985515054415316, param: {'activation': 'softplus'}\n",
      "mean:0.9718095461527506, std:0.0009060092194391034, param: {'activation': 'softsign'}\n",
      "mean:0.9759999910990397, std:0.0004982895954796077, param: {'activation': 'relu'}\n",
      "mean:0.9734285672505697, std:0.0009385755274574532, param: {'activation': 'tanh'}\n",
      "mean:0.9712619185447693, std:0.0006450539516603992, param: {'activation': 'sigmoid'}\n",
      "mean:0.9705952405929565, std:0.0006759414925494781, param: {'activation': 'hard_sigmoid'}\n",
      "mean:0.9095714092254639, std:0.0023350276423606993, param: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_a.best_score_}\\nbest parameters {grid_result_a.best_params_}')\n",
    "means_a = grid_result_a.cv_results_['mean_test_score']\n",
    "stds_a = grid_result_a.cv_results_['std_test_score']\n",
    "params_a = grid_result_a.cv_results_['params']\n",
    "for mean, std, param in zip(means_a, stds_a, params_a):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 10s 300us/sample - loss: 0.2545 - acc: 0.9222 - val_loss: 0.1421 - val_acc: 0.9561\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.1051 - acc: 0.9691 - val_loss: 0.0980 - val_acc: 0.9698\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 0.0679 - acc: 0.9800 - val_loss: 0.0831 - val_acc: 0.9720\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0487 - acc: 0.9862 - val_loss: 0.0845 - val_acc: 0.9740\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 0.0357 - acc: 0.9899 - val_loss: 0.0784 - val_acc: 0.9767\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0257 - acc: 0.9933 - val_loss: 0.0781 - val_acc: 0.9780\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0736 - val_acc: 0.9781\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0137 - acc: 0.9967 - val_loss: 0.0792 - val_acc: 0.9777\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0096 - acc: 0.9980 - val_loss: 0.0778 - val_acc: 0.9786\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 0.0064 - acc: 0.9992 - val_loss: 0.0835 - val_acc: 0.9771\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0822 - val_acc: 0.9787\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0034 - acc: 0.9996 - val_loss: 0.0780 - val_acc: 0.9799\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.0794 - val_acc: 0.9802\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0840 - val_acc: 0.9793\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0802 - val_acc: 0.9802\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0800 - val_acc: 0.9806\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0826 - val_acc: 0.9795\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 10s 287us/sample - loss: 9.3289e-04 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9805\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 8.7951e-04 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9800\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 10s 291us/sample - loss: 8.1914e-04 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9800\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 7.7655e-04 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9808\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 7.4854e-04 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9804\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 7.2366e-04 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9805\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 7.0232e-04 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9805\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 6.8987e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9804\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 6.7498e-04 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9808\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 6.6097e-04 - acc: 1.0000 - val_loss: 0.0884 - val_acc: 0.9804\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.4948e-04 - acc: 1.0000 - val_loss: 0.0887 - val_acc: 0.9799\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 6.3909e-04 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9808\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.3042e-04 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9801\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 6.2296e-04 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9800\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.1583e-04 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9806\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 6.0977e-04 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9801\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 6.0340e-04 - acc: 1.0000 - val_loss: 0.0903 - val_acc: 0.9800\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 5.9831e-04 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 0.9801\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.9316e-04 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 0.9801\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.8858e-04 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9801\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 5.8463e-04 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9801\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.8095e-04 - acc: 1.0000 - val_loss: 0.0919 - val_acc: 0.9805\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.7710e-04 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9801\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.7363e-04 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9804\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.7047e-04 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9800\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 10s 287us/sample - loss: 5.6766e-04 - acc: 1.0000 - val_loss: 0.0928 - val_acc: 0.9802\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.6506e-04 - acc: 1.0000 - val_loss: 0.0930 - val_acc: 0.9801\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.6206e-04 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9805\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.5991e-04 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9799\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.5773e-04 - acc: 1.0000 - val_loss: 0.0937 - val_acc: 0.9801\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.5549e-04 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 0.9802\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 5.5330e-04 - acc: 1.0000 - val_loss: 0.0942 - val_acc: 0.9802\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.5115e-04 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9800\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.4939e-04 - acc: 1.0000 - val_loss: 0.0945 - val_acc: 0.9802\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.4773e-04 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9800\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 9s 280us/sample - loss: 5.4598e-04 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9801\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 5.4447e-04 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9800\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.4269e-04 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9802\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.4156e-04 - acc: 1.0000 - val_loss: 0.0953 - val_acc: 0.9802\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.4015e-04 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9800\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.3871e-04 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 0.9801\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 5.3743e-04 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9800\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.3622e-04 - acc: 1.0000 - val_loss: 0.0957 - val_acc: 0.9801\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.3494e-04 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9800\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 10s 284us/sample - loss: 5.3390e-04 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9802\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.3282e-04 - acc: 1.0000 - val_loss: 0.0962 - val_acc: 0.9799\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 10s 287us/sample - loss: 5.3173e-04 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9799\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 9s 282us/sample - loss: 5.3066e-04 - acc: 1.0000 - val_loss: 0.0966 - val_acc: 0.9800\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 10s 286us/sample - loss: 5.2978e-04 - acc: 1.0000 - val_loss: 0.0966 - val_acc: 0.9800\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.2889e-04 - acc: 1.0000 - val_loss: 0.0968 - val_acc: 0.9800\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 9s 283us/sample - loss: 5.2795e-04 - acc: 1.0000 - val_loss: 0.0967 - val_acc: 0.9800\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.2699e-04 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9801\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.2617e-04 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9800\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.2534e-04 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9804\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.2456e-04 - acc: 1.0000 - val_loss: 0.0973 - val_acc: 0.9799\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 9s 274us/sample - loss: 5.2386e-04 - acc: 1.0000 - val_loss: 0.0976 - val_acc: 0.9800\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.2302e-04 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9804\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 9s 278us/sample - loss: 5.2234e-04 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9800\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 10s 283us/sample - loss: 5.2164e-04 - acc: 1.0000 - val_loss: 0.0977 - val_acc: 0.9801\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.2105e-04 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9800\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 10s 285us/sample - loss: 5.2027e-04 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 0.9800\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.1973e-04 - acc: 1.0000 - val_loss: 0.0981 - val_acc: 0.9799\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 5.1908e-04 - acc: 1.0000 - val_loss: 0.0983 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "def create_model(activation='relu'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, activation='relu'))\n",
    "\tmodel.add(Dense(10, activation=activation))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_a = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9759761889775594\n",
      "best parameters {'activation': 'softmax'}\n",
      "mean:0.9759761889775594, std:0.0005841973974428128, param: {'activation': 'softmax'}\n",
      "mean:0.9759047627449036, std:0.00047500718906613157, param: {'activation': 'softplus'}\n",
      "mean:0.22033332784970602, std:0.05869528940423973, param: {'activation': 'softsign'}\n",
      "mean:0.09838095307350159, std:0.0018073304764158126, param: {'activation': 'relu'}\n",
      "mean:0.12490476171175639, std:0.036148612544192035, param: {'activation': 'tanh'}\n",
      "mean:0.09838095307350159, std:0.0018073304764158126, param: {'activation': 'sigmoid'}\n",
      "mean:0.2926666736602783, std:0.019851345562555682, param: {'activation': 'hard_sigmoid'}\n",
      "mean:0.2731904735167821, std:0.05014795015683855, param: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_a.best_score_}\\nbest parameters {grid_result_a.best_params_}')\n",
    "means_a = grid_result_a.cv_results_['mean_test_score']\n",
    "stds_a = grid_result_a.cv_results_['std_test_score']\n",
    "params_a = grid_result_a.cv_results_['params']\n",
    "for mean, std, param in zip(means_a, stds_a, params_a):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 15s 447us/sample - loss: 2.1740 - acc: 0.2082 - val_loss: 1.8845 - val_acc: 0.3190\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 14s 412us/sample - loss: 1.6657 - acc: 0.3461 - val_loss: 1.6014 - val_acc: 0.3680\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 14s 416us/sample - loss: 1.4514 - acc: 0.4620 - val_loss: 1.2208 - val_acc: 0.5926\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 13s 397us/sample - loss: 1.1115 - acc: 0.6091 - val_loss: 1.0486 - val_acc: 0.6356\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 14s 412us/sample - loss: 1.0101 - acc: 0.6481 - val_loss: 0.9744 - val_acc: 0.6611\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 14s 408us/sample - loss: 0.9460 - acc: 0.6754 - val_loss: 0.9246 - val_acc: 0.6954\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 14s 407us/sample - loss: 0.9042 - acc: 0.7015 - val_loss: 0.8961 - val_acc: 0.7092\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 14s 410us/sample - loss: 0.8750 - acc: 0.7222 - val_loss: 0.8727 - val_acc: 0.7248\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 14s 410us/sample - loss: 0.8519 - acc: 0.7312 - val_loss: 0.8485 - val_acc: 0.7505\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 14s 412us/sample - loss: 0.8299 - acc: 0.7427 - val_loss: 0.8428 - val_acc: 0.7323\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 14s 411us/sample - loss: 0.8120 - acc: 0.7456 - val_loss: 0.8139 - val_acc: 0.7463\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 14s 411us/sample - loss: 0.7939 - acc: 0.7512 - val_loss: 0.8024 - val_acc: 0.7480\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 15s 433us/sample - loss: 0.7773 - acc: 0.7594 - val_loss: 0.7949 - val_acc: 0.7451\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 14s 407us/sample - loss: 0.7604 - acc: 0.7687 - val_loss: 0.7862 - val_acc: 0.7562\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 15s 447us/sample - loss: 0.7452 - acc: 0.7767 - val_loss: 0.7608 - val_acc: 0.7676\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 15s 452us/sample - loss: 0.7318 - acc: 0.7829 - val_loss: 0.7431 - val_acc: 0.7795\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 16s 462us/sample - loss: 0.7174 - acc: 0.7929 - val_loss: 0.7283 - val_acc: 0.7913\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 16s 465us/sample - loss: 0.7052 - acc: 0.7967 - val_loss: 0.7254 - val_acc: 0.7868\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 17s 494us/sample - loss: 0.6947 - acc: 0.8012 - val_loss: 0.7201 - val_acc: 0.7963\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 16s 465us/sample - loss: 0.6853 - acc: 0.8039 - val_loss: 0.6977 - val_acc: 0.8067\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 16s 489us/sample - loss: 0.6765 - acc: 0.8085 - val_loss: 0.6913 - val_acc: 0.8068\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 16s 476us/sample - loss: 0.6685 - acc: 0.8097 - val_loss: 0.6827 - val_acc: 0.8073\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 18s 533us/sample - loss: 0.6607 - acc: 0.8143 - val_loss: 0.6773 - val_acc: 0.8136\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 16s 467us/sample - loss: 0.6542 - acc: 0.8158 - val_loss: 0.6738 - val_acc: 0.8098\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 16s 466us/sample - loss: 0.6486 - acc: 0.8179 - val_loss: 0.6667 - val_acc: 0.8114\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 15s 457us/sample - loss: 0.6424 - acc: 0.8196 - val_loss: 0.6808 - val_acc: 0.8052\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 16s 472us/sample - loss: 0.6355 - acc: 0.8235 - val_loss: 0.6538 - val_acc: 0.8162\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 17s 498us/sample - loss: 0.6298 - acc: 0.8253 - val_loss: 0.6541 - val_acc: 0.8162\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 16s 469us/sample - loss: 0.6252 - acc: 0.8263 - val_loss: 0.6474 - val_acc: 0.8204\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 16s 477us/sample - loss: 0.6204 - acc: 0.8283 - val_loss: 0.6522 - val_acc: 0.8104\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 16s 471us/sample - loss: 0.6156 - acc: 0.8293 - val_loss: 0.6471 - val_acc: 0.8239\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 17s 516us/sample - loss: 0.6124 - acc: 0.8296 - val_loss: 0.6346 - val_acc: 0.8264\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 17s 491us/sample - loss: 0.6085 - acc: 0.8319 - val_loss: 0.6303 - val_acc: 0.8252\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 16s 481us/sample - loss: 0.6057 - acc: 0.8321 - val_loss: 0.7073 - val_acc: 0.7763\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 16s 484us/sample - loss: 0.6024 - acc: 0.8321 - val_loss: 0.6247 - val_acc: 0.8254\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 14s 409us/sample - loss: 0.5992 - acc: 0.8342 - val_loss: 0.6296 - val_acc: 0.8315\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 12s 372us/sample - loss: 0.5963 - acc: 0.8339 - val_loss: 0.6271 - val_acc: 0.8265\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 12s 372us/sample - loss: 0.5937 - acc: 0.8340 - val_loss: 0.6170 - val_acc: 0.8333\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 13s 373us/sample - loss: 0.5909 - acc: 0.8349 - val_loss: 0.6168 - val_acc: 0.8332\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 13s 375us/sample - loss: 0.5888 - acc: 0.8341 - val_loss: 0.6165 - val_acc: 0.8301\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 15s 460us/sample - loss: 0.5863 - acc: 0.8362 - val_loss: 0.6112 - val_acc: 0.8305\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 16s 477us/sample - loss: 0.5843 - acc: 0.8359 - val_loss: 0.6054 - val_acc: 0.8327\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 15s 460us/sample - loss: 0.5831 - acc: 0.8362 - val_loss: 0.6038 - val_acc: 0.8332\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 16s 475us/sample - loss: 0.5798 - acc: 0.8384 - val_loss: 0.6240 - val_acc: 0.8182\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 16s 469us/sample - loss: 0.5785 - acc: 0.8379 - val_loss: 0.6258 - val_acc: 0.8260\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 18s 525us/sample - loss: 0.5756 - acc: 0.8365 - val_loss: 0.6161 - val_acc: 0.8175\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 16s 462us/sample - loss: 0.5743 - acc: 0.8390 - val_loss: 0.6071 - val_acc: 0.8324\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 16s 479us/sample - loss: 0.5726 - acc: 0.8385 - val_loss: 0.6036 - val_acc: 0.8292\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 15s 461us/sample - loss: 0.5707 - acc: 0.8400 - val_loss: 0.6032 - val_acc: 0.8346\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 16s 477us/sample - loss: 0.5674 - acc: 0.8385 - val_loss: 0.6017 - val_acc: 0.8273\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 15s 460us/sample - loss: 0.5665 - acc: 0.8407 - val_loss: 0.5956 - val_acc: 0.8329\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 16s 465us/sample - loss: 0.5663 - acc: 0.8389 - val_loss: 0.6045 - val_acc: 0.8351\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 16s 473us/sample - loss: 0.5634 - acc: 0.8404 - val_loss: 0.6012 - val_acc: 0.8295\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 15s 461us/sample - loss: 0.5623 - acc: 0.8402 - val_loss: 0.6012 - val_acc: 0.8281\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 16s 476us/sample - loss: 0.5614 - acc: 0.8410 - val_loss: 0.5912 - val_acc: 0.8327\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 15s 461us/sample - loss: 0.5605 - acc: 0.8429 - val_loss: 0.5821 - val_acc: 0.8361\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 15s 456us/sample - loss: 0.5585 - acc: 0.8411 - val_loss: 0.6042 - val_acc: 0.8293\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 15s 441us/sample - loss: 0.5567 - acc: 0.8412 - val_loss: 0.5961 - val_acc: 0.8326\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 15s 439us/sample - loss: 0.5564 - acc: 0.8409 - val_loss: 0.5812 - val_acc: 0.8390\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 15s 439us/sample - loss: 0.5548 - acc: 0.8422 - val_loss: 0.5975 - val_acc: 0.8294\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 15s 436us/sample - loss: 0.5528 - acc: 0.8440 - val_loss: 0.6213 - val_acc: 0.8251\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 15s 442us/sample - loss: 0.5506 - acc: 0.8449 - val_loss: 0.5974 - val_acc: 0.8274\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 15s 454us/sample - loss: 0.5502 - acc: 0.8439 - val_loss: 0.5962 - val_acc: 0.8304\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 15s 451us/sample - loss: 0.5497 - acc: 0.8438 - val_loss: 0.5817 - val_acc: 0.8386\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 15s 454us/sample - loss: 0.5484 - acc: 0.8451 - val_loss: 0.6131 - val_acc: 0.8268\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 15s 458us/sample - loss: 0.5478 - acc: 0.8435 - val_loss: 0.5749 - val_acc: 0.8399\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 15s 457us/sample - loss: 0.5467 - acc: 0.8439 - val_loss: 0.5844 - val_acc: 0.8315\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 15s 459us/sample - loss: 0.5448 - acc: 0.8456 - val_loss: 0.5749 - val_acc: 0.8414\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 16s 472us/sample - loss: 0.5445 - acc: 0.8457 - val_loss: 0.5734 - val_acc: 0.8417\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 15s 460us/sample - loss: 0.5431 - acc: 0.8457 - val_loss: 0.5827 - val_acc: 0.8343\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 15s 459us/sample - loss: 0.5431 - acc: 0.8475 - val_loss: 0.5781 - val_acc: 0.8401\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 16s 463us/sample - loss: 0.5420 - acc: 0.8467 - val_loss: 0.5758 - val_acc: 0.8380\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 16s 465us/sample - loss: 0.5407 - acc: 0.8457 - val_loss: 0.5777 - val_acc: 0.8377\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 16s 467us/sample - loss: 0.5400 - acc: 0.8478 - val_loss: 0.6255 - val_acc: 0.8126\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 15s 443us/sample - loss: 0.5399 - acc: 0.8458 - val_loss: 0.5871 - val_acc: 0.8339\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 15s 447us/sample - loss: 0.5390 - acc: 0.8456 - val_loss: 0.5680 - val_acc: 0.8424\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 15s 444us/sample - loss: 0.5382 - acc: 0.8480 - val_loss: 0.5891 - val_acc: 0.8318\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 15s 441us/sample - loss: 0.5366 - acc: 0.8472 - val_loss: 0.5775 - val_acc: 0.8389\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 15s 452us/sample - loss: 0.5379 - acc: 0.8473 - val_loss: 0.5771 - val_acc: 0.8363\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 15s 447us/sample - loss: 0.5358 - acc: 0.8491 - val_loss: 0.5751 - val_acc: 0.8407\n"
     ]
    }
   ],
   "source": [
    "def create_model(activation='relu'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, activation=activation))\n",
    "\tmodel.add(Dense(10, activation=activation))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)\n",
    "\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_a = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.711214284102122\n",
      "best parameters {'activation': 'softmax'}\n",
      "mean:0.711214284102122, std:0.0437031763473945, param: {'activation': 'softmax'}\n",
      "mean:0.38952381412188214, std:0.4103592993439094, param: {'activation': 'softplus'}\n",
      "mean:0.2216190497080485, std:0.08070278886082105, param: {'activation': 'softsign'}\n",
      "mean:0.09838095307350159, std:0.0018073304764158126, param: {'activation': 'relu'}\n",
      "mean:0.09838095307350159, std:0.0018073304764158126, param: {'activation': 'tanh'}\n",
      "mean:0.09838095307350159, std:0.0018073304764158126, param: {'activation': 'sigmoid'}\n",
      "mean:0.32964285214742023, std:0.022065760059470158, param: {'activation': 'hard_sigmoid'}\n",
      "mean:0.17957143237193426, std:0.1118747040810211, param: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_a.best_score_}\\nbest parameters {grid_result_a.best_params_}')\n",
    "means_a = grid_result_a.cv_results_['mean_test_score']\n",
    "stds_a = grid_result_a.cv_results_['std_test_score']\n",
    "params_a = grid_result_a.cv_results_['params']\n",
    "for mean, std, param in zip(means_a, stds_a, params_a):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function: softmax\n",
    "\n",
    "best score : 0.711214284102122\n",
    "\n",
    "loss: 0.5358 - acc: 0.8491 - val_loss: 0.5751 - val_acc: 0.8407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning drop out regularization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined with weights constraints\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(800, input_dim=784, activation='relu',\n",
    "                    kernel_constraint=max_norm(weight_constraint)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qu4n7\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 13s 401us/sample - loss: 0.3251 - acc: 0.9034 - val_loss: 0.1726 - val_acc: 0.9486\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 12s 360us/sample - loss: 0.1671 - acc: 0.9516 - val_loss: 0.1180 - val_acc: 0.9638\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 13s 374us/sample - loss: 0.1227 - acc: 0.9632 - val_loss: 0.1054 - val_acc: 0.9679\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 13s 383us/sample - loss: 0.1018 - acc: 0.9710 - val_loss: 0.0964 - val_acc: 0.9707\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0862 - acc: 0.9741 - val_loss: 0.0894 - val_acc: 0.9725\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0768 - acc: 0.9770 - val_loss: 0.0849 - val_acc: 0.9743\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0666 - acc: 0.9804 - val_loss: 0.0886 - val_acc: 0.9726\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0607 - acc: 0.9812 - val_loss: 0.0819 - val_acc: 0.9762\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0576 - acc: 0.9826 - val_loss: 0.0797 - val_acc: 0.9769\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 13s 383us/sample - loss: 0.0492 - acc: 0.9847 - val_loss: 0.0801 - val_acc: 0.9761\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 13s 383us/sample - loss: 0.0449 - acc: 0.9864 - val_loss: 0.0815 - val_acc: 0.9765\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0404 - acc: 0.9873 - val_loss: 0.0801 - val_acc: 0.9790\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0386 - acc: 0.9882 - val_loss: 0.0787 - val_acc: 0.9785\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0357 - acc: 0.9887 - val_loss: 0.0775 - val_acc: 0.9779\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0339 - acc: 0.9899 - val_loss: 0.0774 - val_acc: 0.9774\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0309 - acc: 0.9905 - val_loss: 0.0772 - val_acc: 0.9787\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0766 - val_acc: 0.9792\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 13s 382us/sample - loss: 0.0262 - acc: 0.9910 - val_loss: 0.0774 - val_acc: 0.9794\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0257 - acc: 0.9918 - val_loss: 0.0766 - val_acc: 0.9800\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0234 - acc: 0.9920 - val_loss: 0.0775 - val_acc: 0.9801\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 13s 383us/sample - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0800 - val_acc: 0.9796\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0219 - acc: 0.9930 - val_loss: 0.0775 - val_acc: 0.9798\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 13s 379us/sample - loss: 0.0198 - acc: 0.9934 - val_loss: 0.0757 - val_acc: 0.9813\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 12s 362us/sample - loss: 0.0209 - acc: 0.9934 - val_loss: 0.0756 - val_acc: 0.9812\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 12s 359us/sample - loss: 0.0181 - acc: 0.9944 - val_loss: 0.0786 - val_acc: 0.9812\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 13s 381us/sample - loss: 0.0172 - acc: 0.9946 - val_loss: 0.0802 - val_acc: 0.9815\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0153 - acc: 0.9950 - val_loss: 0.0783 - val_acc: 0.9814\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 13s 380us/sample - loss: 0.0148 - acc: 0.9954 - val_loss: 0.0806 - val_acc: 0.9807\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0796 - val_acc: 0.9821\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.0778 - val_acc: 0.9810\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0131 - acc: 0.9962 - val_loss: 0.0842 - val_acc: 0.9799\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0136 - acc: 0.9960 - val_loss: 0.0831 - val_acc: 0.9808\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0821 - val_acc: 0.9814\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0813 - val_acc: 0.9815\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0113 - acc: 0.9969 - val_loss: 0.0815 - val_acc: 0.9811\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0859 - val_acc: 0.9801\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0836 - val_acc: 0.9808\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0850 - val_acc: 0.9818\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0842 - val_acc: 0.9819\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0829 - val_acc: 0.9817\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0806 - val_acc: 0.9817\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0837 - val_acc: 0.9818\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0867 - val_acc: 0.9812\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0851 - val_acc: 0.9814\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0896 - val_acc: 0.9806\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 12s 371us/sample - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0849 - val_acc: 0.9819\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 12s 361us/sample - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0836 - val_acc: 0.9824\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 12s 364us/sample - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0843 - val_acc: 0.9825\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0856 - val_acc: 0.9817\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 13s 391us/sample - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0899 - val_acc: 0.9815\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0882 - val_acc: 0.9813\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0897 - val_acc: 0.9810\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0873 - val_acc: 0.9819\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 13s 384us/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0885 - val_acc: 0.9832\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0860 - val_acc: 0.9825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 13s 390us/sample - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0874 - val_acc: 0.9824\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0891 - val_acc: 0.9825\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0877 - val_acc: 0.9825\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0896 - val_acc: 0.9824\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0928 - val_acc: 0.9823\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 13s 390us/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.0910 - val_acc: 0.9824\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0913 - val_acc: 0.9819\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 13s 387us/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0949 - val_acc: 0.9817\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0935 - val_acc: 0.9818\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.0882 - val_acc: 0.9825\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0933 - val_acc: 0.9814\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0901 - val_acc: 0.9823\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 13s 380us/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0963 - val_acc: 0.9820\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 12s 362us/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0934 - val_acc: 0.9817\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 12s 363us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0891 - val_acc: 0.9817\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 13s 385us/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0897 - val_acc: 0.9819\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 13s 390us/sample - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0896 - val_acc: 0.9820\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0896 - val_acc: 0.9819\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 13s 389us/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0925 - val_acc: 0.9824\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0920 - val_acc: 0.9826\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 13s 391us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0941 - val_acc: 0.9820\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0914 - val_acc: 0.9819\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0960 - val_acc: 0.9814\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 13s 386us/sample - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0968 - val_acc: 0.9814\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 13s 388us/sample - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0928 - val_acc: 0.9818\n"
     ]
    }
   ],
   "source": [
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_d = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9788333376248678\n",
      "best parameters {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "mean:0.9765476187070211, std:0.0009041219112741232, param: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "mean:0.976023813088735, std:0.0010240866845850978, param: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "mean:0.9759999910990397, std:0.0006172238479547994, param: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "mean:0.9761190414428711, std:0.0006424004399827392, param: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "mean:0.9760476350784302, std:0.0008735259324273799, param: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "mean:0.976809541384379, std:0.0010190992932132624, param: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "mean:0.9770714243253072, std:0.0005744210694721152, param: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "mean:0.9768809676170349, std:0.0005724384964749627, param: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "mean:0.9763333400090536, std:0.0005418754850322328, param: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "mean:0.9766190449396769, std:0.0006424284218288155, param: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "mean:0.977738082408905, std:0.0010790617437376607, param: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "mean:0.97730952501297, std:0.0006555173043820095, param: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "mean:0.9770714441935221, std:0.0003547556340489494, param: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "mean:0.976976195971171, std:0.00034175344964973545, param: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "mean:0.9771666526794434, std:0.0004856257868604565, param: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "mean:0.9781190554300944, std:0.0006424107491287045, param: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "mean:0.9780476093292236, std:0.0003563384248215155, param: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "mean:0.9775952498118082, std:0.0007030875644503293, param: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "mean:0.9777857263882955, std:0.0002672790412033867, param: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "mean:0.9780952334403992, std:0.0006734223455070648, param: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "mean:0.9780714313189188, std:0.0005563465791847388, param: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "mean:0.9782619078954061, std:0.0005543053368057874, param: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "mean:0.9779523809750875, std:0.0009337477767261131, param: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "mean:0.9787380894025167, std:0.000584176343401069, param: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "mean:0.9779761830965678, std:0.00045299468943208924, param: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "mean:0.9783095320065817, std:0.0010190788689288766, param: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "mean:0.9785000085830688, std:0.0004205575192569244, param: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "mean:0.9782381057739258, std:0.0007875150151834275, param: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "mean:0.9784761667251587, std:0.0010438238055568004, param: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "mean:0.9788333376248678, std:0.000429905946195301, param: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "mean:0.9780714313189188, std:0.0006308396533910653, param: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "mean:0.9783809582392374, std:0.0006759421921742617, param: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "mean:0.9782857100168864, std:0.0005183649158964571, param: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "mean:0.9778809547424316, std:0.0009227639033975983, param: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "mean:0.9781428774197897, std:0.0012051510145243754, param: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "mean:0.9771190484364828, std:0.0008598019880105722, param: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "mean:0.9775238037109375, std:0.0003417257640344799, param: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "mean:0.9771190683046976, std:0.0005419025466950435, param: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "mean:0.977571427822113, std:0.0010895100274313225, param: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "mean:0.9768571456273397, std:0.0008017787133046922, param: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "mean:0.9754047592480978, std:0.0004960323757391313, param: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "mean:0.9760952591896057, std:0.0007766416917897308, param: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "mean:0.9759285847345988, std:0.00030306393869233535, param: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "mean:0.9760476350784302, std:0.0005259769339068562, param: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "mean:0.9765000144640604, std:0.0005624384742840946, param: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "mean:0.9667142828305563, std:0.0010801187408223153, param: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "mean:0.9680237968762716, std:0.0008171796513701265, param: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "mean:0.9677380919456482, std:0.0015782766132311971, param: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "mean:0.9676428437232971, std:0.0016913238571275033, param: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "mean:0.967714269955953, std:0.0007846369479592252, param: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_d.best_score_}\\nbest parameters {grid_result_d.best_params_}')\n",
    "means_d = grid_result_d.cv_results_['mean_test_score']\n",
    "stds_d = grid_result_d.cv_results_['std_test_score']\n",
    "params_d = grid_result_d.cv_results_['params']\n",
    "for mean, std, param in zip(means_d, stds_d, params_d):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate: 0.5, weight_constraint: 5\n",
    "\n",
    "best score : 0.9788333376248678\n",
    "\n",
    "loss: 0.0035 - acc: 0.9990 - val_loss: 0.0928 - val_acc: 0.9818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons=1000):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(neurons, input_dim=784,  \n",
    "                    activation='relu', kernel_constraint=max_norm(5)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=80, batch_size=30, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 20s 600us/sample - loss: 0.3017 - acc: 0.9095 - val_loss: 0.1519 - val_acc: 0.9526\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 18s 550us/sample - loss: 0.1503 - acc: 0.9549 - val_loss: 0.1208 - val_acc: 0.9615\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 18s 532us/sample - loss: 0.1113 - acc: 0.9662 - val_loss: 0.1041 - val_acc: 0.9694\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 18s 550us/sample - loss: 0.0898 - acc: 0.9727 - val_loss: 0.0872 - val_acc: 0.9727\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 20s 592us/sample - loss: 0.0736 - acc: 0.9779 - val_loss: 0.0909 - val_acc: 0.9726\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 19s 578us/sample - loss: 0.0639 - acc: 0.9810 - val_loss: 0.0845 - val_acc: 0.9752\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 20s 602us/sample - loss: 0.0566 - acc: 0.9830 - val_loss: 0.0825 - val_acc: 0.9756\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 19s 578us/sample - loss: 0.0486 - acc: 0.9852 - val_loss: 0.0804 - val_acc: 0.9760\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 20s 607us/sample - loss: 0.0437 - acc: 0.9870 - val_loss: 0.0788 - val_acc: 0.9775\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 20s 596us/sample - loss: 0.0405 - acc: 0.9878 - val_loss: 0.0835 - val_acc: 0.9770\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 19s 560us/sample - loss: 0.0357 - acc: 0.9889 - val_loss: 0.0751 - val_acc: 0.9789\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 19s 557us/sample - loss: 0.0308 - acc: 0.9906 - val_loss: 0.0796 - val_acc: 0.9796\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 19s 555us/sample - loss: 0.0300 - acc: 0.9912 - val_loss: 0.0782 - val_acc: 0.9790\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 19s 574us/sample - loss: 0.0277 - acc: 0.9917 - val_loss: 0.0762 - val_acc: 0.9796\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 20s 582us/sample - loss: 0.0235 - acc: 0.9929 - val_loss: 0.0774 - val_acc: 0.9808\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 19s 558us/sample - loss: 0.0217 - acc: 0.9932 - val_loss: 0.0762 - val_acc: 0.9808\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 19s 553us/sample - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0783 - val_acc: 0.9795\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 18s 550us/sample - loss: 0.0186 - acc: 0.9946 - val_loss: 0.0792 - val_acc: 0.9805\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 18s 549us/sample - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0808 - val_acc: 0.9805\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 18s 540us/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0793 - val_acc: 0.9805\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 18s 526us/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0781 - val_acc: 0.9801s - l\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 18s 540us/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0831 - val_acc: 0.9814\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 19s 551us/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.0847 - val_acc: 0.9808\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 18s 546us/sample - loss: 0.0117 - acc: 0.9967 - val_loss: 0.0822 - val_acc: 0.9810\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 18s 550us/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.0816 - val_acc: 0.9820\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 19s 551us/sample - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0854 - val_acc: 0.9807\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 18s 544us/sample - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0790 - val_acc: 0.9817\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 19s 551us/sample - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0834 - val_acc: 0.9812\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 21s 626us/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0877 - val_acc: 0.9824\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 20s 598us/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0907 - val_acc: 0.9826\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 20s 582us/sample - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0918 - val_acc: 0.9821\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 19s 571us/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0911 - val_acc: 0.9830\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 18s 551us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0908 - val_acc: 0.9824\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 20s 587us/sample - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0912 - val_acc: 0.9827\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 19s 578us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0915 - val_acc: 0.9829\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 20s 589us/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0934 - val_acc: 0.9823\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 20s 585us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0905 - val_acc: 0.9825\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 20s 591us/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0909 - val_acc: 0.9820\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 20s 605us/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0945 - val_acc: 0.9827\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 22s 665us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0907 - val_acc: 0.9824\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 21s 636us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0947 - val_acc: 0.9832\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 20s 597us/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0923 - val_acc: 0.9827\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 21s 622us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0921 - val_acc: 0.9825\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 21s 625us/sample - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0901 - val_acc: 0.9831\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 20s 601us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0906 - val_acc: 0.9830\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 21s 615us/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0937 - val_acc: 0.9820\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 21s 640us/sample - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0953 - val_acc: 0.9819\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 22s 657us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0957 - val_acc: 0.9825\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 21s 611us/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0963 - val_acc: 0.9824\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 19s 565us/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0944 - val_acc: 0.9826\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 20s 589us/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0978 - val_acc: 0.9817\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 20s 597us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0972 - val_acc: 0.9829\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 20s 596us/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0948 - val_acc: 0.9819\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 20s 590us/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0964 - val_acc: 0.9823\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 21s 632us/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0943 - val_acc: 0.9829\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 20s 610us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0942 - val_acc: 0.9829\n"
     ]
    }
   ],
   "source": [
    "neurons = [1000, 1300, 1700]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=2)\n",
    "grid_result_n = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.9788809617360433\n",
      "best parameters {'neurons': 1300}\n",
      "mean:0.97773810227712, std:0.0001467744768106376, param: {'neurons': 1000}\n",
      "mean:0.9788809617360433, std:0.0003749648704904566, param: {'neurons': 1300}\n",
      "mean:0.9784285624821981, std:0.0007284347835630508, param: {'neurons': 1700}\n"
     ]
    }
   ],
   "source": [
    "print(f'best score {grid_result_n.best_score_}\\nbest parameters {grid_result_n.best_params_}')\n",
    "means_n = grid_result_n.cv_results_['mean_test_score']\n",
    "stds_n = grid_result_n.cv_results_['std_test_score']\n",
    "params_n = grid_result_n.cv_results_['params']\n",
    "for mean, std, param in zip(means_n, stds_n, params_n):\n",
    "    print(f'mean:{mean}, std:{std}, param: {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neurons number: 1300\n",
    "\n",
    "best score : 0.9788809617360433\n",
    "\n",
    "loss: 0.0018 - acc: 0.9996 - val_loss: 0.0942 - val_acc: 0.9829"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding new layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding the hidden layer based on the optimal hyperparameters for the single layer network \n",
    "* though in the best case the parameters should be grid_serached for 2 layers network separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qu4n7\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\qu4n7\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1300)              1020500   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1300)              1691300   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1300)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                13010     \n",
      "=================================================================\n",
      "Total params: 2,724,810\n",
      "Trainable params: 2,724,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(1300, input_dim=784, \n",
    "                    activation='relu', kernel_constraint=max_norm(5)))\n",
    "\tmodel.add(Dense(1300, \n",
    "                    activation='relu', kernel_constraint=max_norm(5)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "WARNING:tensorflow:From C:\\Users\\qu4n7\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.2432 - acc: 0.9237 - val_loss: 0.1199 - val_acc: 0.9608\n",
      "Epoch 2/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.0953 - acc: 0.9713 - val_loss: 0.0936 - val_acc: 0.9707\n",
      "Epoch 3/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0580 - acc: 0.9818 - val_loss: 0.0798 - val_acc: 0.9751\n",
      "Epoch 4/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 0.0385 - acc: 0.9877 - val_loss: 0.0812 - val_acc: 0.9770\n",
      "Epoch 5/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.0256 - acc: 0.9924 - val_loss: 0.0979 - val_acc: 0.9754\n",
      "Epoch 6/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0189 - acc: 0.9942 - val_loss: 0.0939 - val_acc: 0.9787\n",
      "Epoch 7/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0994 - val_acc: 0.9785\n",
      "Epoch 8/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0089 - acc: 0.9975 - val_loss: 0.0947 - val_acc: 0.9798\n",
      "Epoch 9/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0929 - val_acc: 0.9808\n",
      "Epoch 10/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0937 - val_acc: 0.9817\n",
      "Epoch 11/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.1043 - val_acc: 0.9805\n",
      "Epoch 12/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1102 - val_acc: 0.9800\n",
      "Epoch 13/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1042 - val_acc: 0.9814\n",
      "Epoch 14/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0967 - val_acc: 0.9817\n",
      "Epoch 15/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 7.7039e-04 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9815\n",
      "Epoch 16/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 6.2797e-04 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 0.9825\n",
      "Epoch 17/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 7.7685e-04 - acc: 0.9999 - val_loss: 0.1021 - val_acc: 0.9824\n",
      "Epoch 18/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 6.7790e-04 - acc: 0.9999 - val_loss: 0.1050 - val_acc: 0.9813\n",
      "Epoch 19/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 6.6236e-04 - acc: 0.9999 - val_loss: 0.1087 - val_acc: 0.9813\n",
      "Epoch 20/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 5.9726e-04 - acc: 0.9999 - val_loss: 0.1058 - val_acc: 0.9819\n",
      "Epoch 21/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 5.2805e-04 - acc: 1.0000 - val_loss: 0.1069 - val_acc: 0.9827\n",
      "Epoch 22/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 5.2966e-04 - acc: 1.0000 - val_loss: 0.1045 - val_acc: 0.9823\n",
      "Epoch 23/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 5.1197e-04 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9819\n",
      "Epoch 24/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.5075e-04 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 0.9815\n",
      "Epoch 25/80\n",
      "33600/33600 [==============================] - 46s 1ms/sample - loss: 5.0704e-04 - acc: 1.0000 - val_loss: 0.1059 - val_acc: 0.9820\n",
      "Epoch 26/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.1444e-04 - acc: 1.0000 - val_loss: 0.1056 - val_acc: 0.9821\n",
      "Epoch 27/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.2611e-04 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9823\n",
      "Epoch 28/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.0246e-04 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9829\n",
      "Epoch 29/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9998e-04 - acc: 1.0000 - val_loss: 0.1094 - val_acc: 0.9825\n",
      "Epoch 30/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.4338e-04 - acc: 0.9999 - val_loss: 0.1148 - val_acc: 0.9818\n",
      "Epoch 31/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.4493e-04 - acc: 1.0000 - val_loss: 0.1097 - val_acc: 0.9812\n",
      "Epoch 32/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.1124e-04 - acc: 1.0000 - val_loss: 0.1110 - val_acc: 0.9814\n",
      "Epoch 33/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 5.0244e-04 - acc: 1.0000 - val_loss: 0.1093 - val_acc: 0.9819\n",
      "Epoch 34/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.0238e-04 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 0.9820\n",
      "Epoch 35/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9285e-04 - acc: 1.0000 - val_loss: 0.1127 - val_acc: 0.9818\n",
      "Epoch 36/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9200e-04 - acc: 1.0000 - val_loss: 0.1118 - val_acc: 0.9820\n",
      "Epoch 37/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.6372e-04 - acc: 0.9999 - val_loss: 0.1117 - val_acc: 0.9819\n",
      "Epoch 38/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9914e-04 - acc: 1.0000 - val_loss: 0.1115 - val_acc: 0.9818\n",
      "Epoch 39/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9524e-04 - acc: 1.0000 - val_loss: 0.1113 - val_acc: 0.9818\n",
      "Epoch 40/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8973e-04 - acc: 1.0000 - val_loss: 0.1103 - val_acc: 0.9819\n",
      "Epoch 41/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9088e-04 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 0.9819\n",
      "Epoch 42/80\n",
      "33600/33600 [==============================] - 43s 1ms/sample - loss: 4.9599e-04 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9823\n",
      "Epoch 43/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9259e-04 - acc: 1.0000 - val_loss: 0.1136 - val_acc: 0.9817\n",
      "Epoch 44/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.9292e-04 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 0.9818\n",
      "Epoch 45/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8939e-04 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.9818\n",
      "Epoch 46/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 4.9456e-04 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.9823\n",
      "Epoch 47/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9321e-04 - acc: 1.0000 - val_loss: 0.1125 - val_acc: 0.9819\n",
      "Epoch 48/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8811e-04 - acc: 1.0000 - val_loss: 0.1117 - val_acc: 0.9821\n",
      "Epoch 49/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8963e-04 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 0.9830\n",
      "Epoch 50/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8878e-04 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 0.9825\n",
      "Epoch 51/80\n",
      "33600/33600 [==============================] - 42s 1ms/sample - loss: 4.9354e-04 - acc: 1.0000 - val_loss: 0.1141 - val_acc: 0.9831\n",
      "Epoch 52/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.9128e-04 - acc: 1.0000 - val_loss: 0.1137 - val_acc: 0.9824\n",
      "Epoch 53/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.9604e-04 - acc: 1.0000 - val_loss: 0.1111 - val_acc: 0.9826\n",
      "Epoch 54/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9066e-04 - acc: 1.0000 - val_loss: 0.1132 - val_acc: 0.9824\n",
      "Epoch 55/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9738e-04 - acc: 1.0000 - val_loss: 0.1139 - val_acc: 0.9827\n",
      "Epoch 56/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.1450e-04 - acc: 1.0000 - val_loss: 0.1173 - val_acc: 0.9815\n",
      "Epoch 57/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.0765e-04 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 0.9819\n",
      "Epoch 58/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.8970e-04 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 0.9818\n",
      "Epoch 59/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8921e-04 - acc: 1.0000 - val_loss: 0.1174 - val_acc: 0.9823\n",
      "Epoch 60/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.9028e-04 - acc: 1.0000 - val_loss: 0.1165 - val_acc: 0.9823\n",
      "Epoch 61/80\n",
      "33600/33600 [==============================] - 43s 1ms/sample - loss: 4.8868e-04 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9824\n",
      "Epoch 62/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8797e-04 - acc: 1.0000 - val_loss: 0.1143 - val_acc: 0.9825\n",
      "Epoch 63/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8893e-04 - acc: 1.0000 - val_loss: 0.1139 - val_acc: 0.9818\n",
      "Epoch 64/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8720e-04 - acc: 1.0000 - val_loss: 0.1151 - val_acc: 0.9826\n",
      "Epoch 65/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8655e-04 - acc: 1.0000 - val_loss: 0.1158 - val_acc: 0.9826\n",
      "Epoch 66/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8549e-04 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 0.9823\n",
      "Epoch 67/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 5.0322e-04 - acc: 1.0000 - val_loss: 0.1139 - val_acc: 0.9829\n",
      "Epoch 68/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.9052e-04 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9825\n",
      "Epoch 69/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8608e-04 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9823\n",
      "Epoch 70/80\n",
      "33600/33600 [==============================] - 43s 1ms/sample - loss: 4.8702e-04 - acc: 1.0000 - val_loss: 0.1149 - val_acc: 0.9819\n",
      "Epoch 71/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 4.8609e-04 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9823\n",
      "Epoch 72/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8422e-04 - acc: 1.0000 - val_loss: 0.1147 - val_acc: 0.9826\n",
      "Epoch 73/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8612e-04 - acc: 1.0000 - val_loss: 0.1157 - val_acc: 0.9824\n",
      "Epoch 74/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8581e-04 - acc: 1.0000 - val_loss: 0.1149 - val_acc: 0.9823\n",
      "Epoch 75/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8605e-04 - acc: 1.0000 - val_loss: 0.1154 - val_acc: 0.9819\n",
      "Epoch 76/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8484e-04 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9824\n",
      "Epoch 77/80\n",
      "33600/33600 [==============================] - 45s 1ms/sample - loss: 4.8868e-04 - acc: 1.0000 - val_loss: 0.1164 - val_acc: 0.9820\n",
      "Epoch 78/80\n",
      "33600/33600 [==============================] - 47s 1ms/sample - loss: 4.8475e-04 - acc: 1.0000 - val_loss: 0.1161 - val_acc: 0.9825\n",
      "Epoch 79/80\n",
      "33600/33600 [==============================] - 44s 1ms/sample - loss: 5.0815e-04 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9820\n",
      "Epoch 80/80\n",
      "33600/33600 [==============================] - 43s 1ms/sample - loss: 4.8518e-04 - acc: 1.0000 - val_loss: 0.1157 - val_acc: 0.9821\n"
     ]
    }
   ],
   "source": [
    "two_layers = model.fit(x_train, y_train, epochs=80, batch_size=30, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(two_layers.history['acc'], label='accuracy score @ train')\n",
    "plt.plot(two_layers.history['val_acc'], label='accuracy score @ valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 4.8518e-04 - acc: 1.0000 - val_loss: 0.1157 - val_acc: 0.9821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_dataset / 255\n",
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "out = np.column_stack((range(1, predictions.shape[0]+1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('submission.csv', out, header='ImageId,Label', comments='', fmt=\"%d,%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Digit Recognizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/235k [00:00<?, ?B/s]\n",
      "  3%|3         | 8.00k/235k [00:00<00:18, 12.3kB/s]\n",
      " 41%|####      | 96.0k/235k [00:01<00:08, 17.1kB/s]\n",
      " 48%|####7     | 112k/235k [00:01<00:05, 23.2kB/s] \n",
      " 65%|######4   | 152k/235k [00:01<00:02, 32.3kB/s]\n",
      " 78%|#######8  | 184k/235k [00:01<00:01, 44.2kB/s]\n",
      " 99%|#########8| 232k/235k [00:01<00:00, 60.6kB/s]\n",
      "100%|##########| 235k/235k [00:07<00:00, 30.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c digit-recognizer -m \"2 layers not tuned\" -f submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your submission scored 0.97971, which is an improvement of your previous score of 0.97814. Great job!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of neural_course_02_kaggle_mnist.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
